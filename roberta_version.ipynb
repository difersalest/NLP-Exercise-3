{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/didiersalest/NLP_HW3_2/NLP-Exercise-3/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "from transformers import RobertaTokenizer, RobertaModel, RobertaConfig, RobertaForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "from evaluate import load\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#  You can install and import any other libraries if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some Chinese punctuations will be tokenized as [UNK], so we replace them with English ones\n",
    "token_replacement = [\n",
    "    [\"：\" , \":\"],\n",
    "    [\"，\" , \",\"],\n",
    "    [\"“\" , \"\\\"\"],\n",
    "    [\"”\" , \"\\\"\"],\n",
    "    [\"？\" , \"?\"],\n",
    "    [\"……\" , \"...\"],\n",
    "    [\"！\" , \"!\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\", cache_dir=\"./cache/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0, 47446,     5,  4195,     4,     2,     2, 47446,   200,  3645,\n",
       "             4,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(text=\"Testing the output.\", text_pair=\"Testing second sentence.\", padding = True, truncation=True, return_tensors=\"pt\", return_attention_mask=True, return_token_type_ids=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 87.3k/87.3k [00:00<00:00, 124kB/s] \n",
      "Downloading data: 100%|██████████| 93.4k/93.4k [00:00<00:00, 131kB/s] \n",
      "Downloading data: 100%|██████████| 16.4k/16.4k [00:00<00:00, 32.1MB/s]\n",
      "Generating train split: 100%|██████████| 4500/4500 [00:00<00:00, 8934.71 examples/s]\n",
      "Generating test split: 100%|██████████| 4927/4927 [00:00<00:00, 33425.59 examples/s]\n",
      "Generating validation split: 100%|██████████| 500/500 [00:00<00:00, 27384.40 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset example: \n",
      "{'sentence_pair_id': 1, 'premise': 'A group of kids is playing in a yard and an old man is standing in the background', 'hypothesis': 'A group of boys in a yard is playing and a man is standing in the background', 'relatedness_score': 4.5, 'entailment_judgment': 0} \n",
      "{'sentence_pair_id': 2, 'premise': 'A group of children is playing in the house and there is no man standing in the background', 'hypothesis': 'A group of kids is playing in a yard and an old man is standing in the background', 'relatedness_score': 3.200000047683716, 'entailment_judgment': 0} \n",
      "{'sentence_pair_id': 3, 'premise': 'The young boys are playing outdoors and the man is smiling nearby', 'hypothesis': 'The kids are playing outdoors near a man with a smile', 'relatedness_score': 4.699999809265137, 'entailment_judgment': 1}\n"
     ]
    }
   ],
   "source": [
    "class SemevalDataset(Dataset):\n",
    "    def __init__(self, split=\"train\") -> None:\n",
    "        super().__init__()\n",
    "        assert split in [\"train\", \"validation\", \"test\"]\n",
    "        self.data = load_dataset(\n",
    "            \"sem_eval_2014_task_1\", split=split, trust_remote_code=True, cache_dir=\"./cache/\"\n",
    "        ).to_list()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        d = self.data[index]\n",
    "        # Replace Chinese punctuations with English ones\n",
    "        for k in [\"premise\", \"hypothesis\"]:\n",
    "            for tok in token_replacement:\n",
    "                d[k] = d[k].replace(tok[0], tok[1])\n",
    "             \n",
    "        return d\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "data_sample = SemevalDataset(split=\"train\").data[:3]\n",
    "print(f\"Dataset example: \\n{data_sample[0]} \\n{data_sample[1]} \\n{data_sample[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters\n",
    "# You can modify these values if needed\n",
    "lr = 3e-5\n",
    "epochs = 3\n",
    "train_batch_size = 8\n",
    "validation_batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/didiersalest/NLP_HW3_2/NLP-Exercise-3/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 16, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# TODO1: Create batched data for DataLoader\n",
    "# `collate_fn` is a function that defines how the data batch should be packed.\n",
    "# This function will be called in the DataLoader to pack the data batch.\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # TODO1-1: Implement the collate_fn function\n",
    "    # Write your code here\n",
    "    # The input parameter is a data batch (tuple), and this function packs it into tensors.\n",
    "    # Use tokenizer to pack tokenize and pack the data and its corresponding labels.\n",
    "    # Return the data batch and labels for each sub-task.\n",
    "    premises = [data_instance[\"premise\"] for data_instance in batch]\n",
    "    hypothesis = [data_instance[\"hypothesis\"] for data_instance in batch]\n",
    "    relatedness_scores = [data_instance[\"relatedness_score\"] for data_instance in batch]\n",
    "    entailment_judgments = [data_instance[\"entailment_judgment\"] for data_instance in batch]\n",
    "\n",
    "    input_texts = tokenizer(premises, hypothesis, padding = True, truncation=True, return_tensors=\"pt\", return_attention_mask=True, return_token_type_ids=False)\n",
    "\n",
    "    relatedness_scores=torch.FloatTensor(relatedness_scores)\n",
    "    entailment_judgments=torch.LongTensor(entailment_judgments)\n",
    "    # print(set(entailment_judgments.numpy()))\n",
    "    return input_texts, relatedness_scores, entailment_judgments\n",
    "\n",
    "# TODO1-2: Define your DataLoader\n",
    "dl_train = torch.utils.data.DataLoader(dataset=SemevalDataset(split=\"train\"), collate_fn=collate_fn, batch_size=train_batch_size, shuffle=True, num_workers=32) # Write your code here\n",
    "dl_validation = torch.utils.data.DataLoader(dataset=SemevalDataset(split=\"validation\"), collate_fn=collate_fn, batch_size=validation_batch_size, shuffle=False, num_workers=32)  # Write your code here\n",
    "dl_test = torch.utils.data.DataLoader(dataset=SemevalDataset(split=\"test\"), collate_fn=collate_fn, batch_size=validation_batch_size, shuffle=False, num_workers=32) # Write your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'input_ids': tensor([[    0,   250,   313,    16,   816,     5,  8669,  1945,     2,     2,\n",
      "           250,   621,    16,   816,    10, 32909,     2,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1],\n",
      "        [    0,   250,   664,   693,    16,   562,    10, 12904,    15,    69,\n",
      "           865,     2,     2,   250,  1816,    16,   562,    10, 12904,    15,\n",
      "            69,   865,     2,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1],\n",
      "        [    0,   970,    16,   117,  4758, 10601, 42922,  4835,  5803,     2,\n",
      "             2,   133,  4758,    16, 10601, 42922,  4835,  5803,     2,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1],\n",
      "        [    0,   250,   313,    16,   816,     5,  8669,     2,     2,   250,\n",
      "           621,    16,  6288,     7,    10, 32909,   816,     2,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1],\n",
      "        [    0,   250,   313,     8,    80,   390,    11,    10, 38390,   929,\n",
      "            32,  2828,    23,    10,  2103,    19, 21702,     2,     2,   133,\n",
      "           333,     9,    82,    16,    45,  2828,    11,    10, 14548,   929,\n",
      "             2],\n",
      "        [    0,   133,   595,    16,   145, 31333,    30,    10,   333,     9,\n",
      "           964,    11,    10, 18992,     2,     2,   250,   333,     9,   964,\n",
      "            32,  5793,     5,   595,    11,    10, 18992,     2,     1,     1,\n",
      "             1],\n",
      "        [    0,   250,   650, 25684,    16,  3051,    11,     5,  4908,     2,\n",
      "             2,   250, 25684,    16,   885,  8423,   149,    10, 16377,     2,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1],\n",
      "        [    0,   250,   739,   313,    16,  2828,    11,    10,   831, 10131,\n",
      "          2792,     2,     2,   250,   739,   313,    16,  2828,    11,    10,\n",
      "           831, 10131,  1400,     2,     1,     1,     1,     1,     1,     1,\n",
      "             1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         0, 0, 0, 0, 0, 0, 0]])}, tensor([2.5000, 4.5000, 3.7000, 1.5000, 3.8000, 4.9000, 3.9000, 5.0000]), tensor([0, 1, 2, 0, 2, 1, 0, 1]))\n"
     ]
    }
   ],
   "source": [
    "print(next(iter(dl_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO2: Construct your model\n",
    "class MultiLabelModel(torch.nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "        # Write your code here\n",
    "        # Define what modules you will use in the model\n",
    "        # Please use \"google-bert/bert-base-uncased\" model (https://huggingface.co/google-bert/bert-base-uncased)\n",
    "        # Besides the base model, you may design additional architectures by incorporating linear layers, activation functions, or other neural components.\n",
    "        # Remark: The use of any additional pretrained language models is not permitted.\n",
    "\n",
    "        num_classes = 3 # Based on our data\n",
    "        # config = RobertaConfig.from_pretrained(\"FacebookAI/roberta-base\", cache_dir=\"./cache/\", pad_token_id=tokenizer.pad_token_id)\n",
    "        # config.max_position_embeddings = 514\n",
    "        \n",
    "        self.roberta_model = RobertaModel.from_pretrained(\n",
    "            \"roberta-base\", \n",
    "            cache_dir=\"./cache/\",\n",
    "            # config=config\n",
    "\n",
    "        )\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "\n",
    "        self.regression_head = torch.nn.Linear(self.roberta_model.config.hidden_size,1)\n",
    "        self.classification_head=torch.nn.Linear(self.roberta_model.config.hidden_size,num_classes)\n",
    "        \n",
    "    def forward(self, **kwargs):\n",
    "        # Write your code here\n",
    "        # Forward pass\n",
    "\n",
    "        output=self.roberta_model(input_ids=kwargs.get(\"input_ids\"), attention_mask=kwargs.get(\"attention_mask\"))\n",
    "        cls_token_output = output.last_hidden_state[:, 0, :]\n",
    "        # output_dropout = self.dropout(output.pooler_output)\n",
    "        output_dropout = self.dropout(cls_token_output)\n",
    "        output_regression = self.regression_head(output_dropout)\n",
    "        output_classification = self.classification_head(output_dropout)\n",
    "\n",
    "        return output_regression, output_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# TODO3: Define your optimizer and loss function\n",
    "\n",
    "model = MultiLabelModel().to(device)\n",
    "# TODO3-1: Define your Optimizer\n",
    "optimizer = torch.optim.AdamW(params =  model.parameters(), lr=lr) # Write your code here\n",
    "\n",
    "# TODO3-2: Define your loss functions (you should have two)\n",
    "# Write your code here\n",
    "classification_criterion = torch.nn.CrossEntropyLoss()\n",
    "regression_criterion = torch.nn.MSELoss()\n",
    "\n",
    "# scoring functions\n",
    "psr = load(\"pearsonr\")\n",
    "acc = load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluationModule(name: \"pearsonr\", module_type: \"metric\", features: {'predictions': Value(dtype='float32', id=None), 'references': Value(dtype='float32', id=None)}, usage: \"\"\"\n",
       "Args:\n",
       "    predictions (`list` of `int`): Predicted class labels, as returned by a model.\n",
       "    references (`list` of `int`): Ground truth labels.\n",
       "    return_pvalue (`boolean`): If `True`, returns the p-value, along with the correlation coefficient. If `False`, returns only the correlation coefficient. Defaults to `False`.\n",
       "\n",
       "Returns:\n",
       "    pearsonr (`float`): Pearson correlation coefficient. Minimum possible value is -1. Maximum possible value is 1. Values of 1 and -1 indicate exact linear positive and negative relationships, respectively. A value of 0 implies no correlation.\n",
       "    p-value (`float`): P-value, which roughly indicates the probability of an The p-value roughly indicates the probability of an uncorrelated system producing datasets that have a Pearson correlation at least as extreme as the one computed from these datasets. Minimum possible value is 0. Maximum possible value is 1. Higher values indicate higher probabilities.\n",
       "\n",
       "Examples:\n",
       "\n",
       "    Example 1-A simple example using only predictions and references.\n",
       "        >>> pearsonr_metric = evaluate.load(\"pearsonr\")\n",
       "        >>> results = pearsonr_metric.compute(predictions=[10, 9, 2.5, 6, 4], references=[1, 2, 3, 4, 5])\n",
       "        >>> print(round(results['pearsonr'], 2))\n",
       "        -0.74\n",
       "\n",
       "    Example 2-The same as Example 1, but that also returns the `p-value`.\n",
       "        >>> pearsonr_metric = evaluate.load(\"pearsonr\")\n",
       "        >>> results = pearsonr_metric.compute(predictions=[10, 9, 2.5, 6, 4], references=[1, 2, 3, 4, 5], return_pvalue=True)\n",
       "        >>> print(sorted(list(results.keys())))\n",
       "        ['p-value', 'pearsonr']\n",
       "        >>> print(round(results['pearsonr'], 2))\n",
       "        -0.74\n",
       "        >>> print(round(results['p-value'], 2))\n",
       "        0.15\n",
       "\"\"\", stored examples: 0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pearsonr': np.float64(0.9997647888947827), 'p-value': np.float64(4.330173202906213e-06)}\n",
      "Pearson correlation coefficient (r): 1.000\n",
      "P-value: 0.000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.array([10, 12, 15, 18, 20])\n",
    "y = np.array([25, 30, 38, 45, 50])\n",
    "# Calculate Pearson correlation and p-value\n",
    "results = psr.compute(references = x, predictions = y, return_pvalue=True)\n",
    "print(results)\n",
    "# Print the results\n",
    "print(f\"Pearson correlation coefficient (r): {results['pearsonr']:.3f}\")\n",
    "print(f\"P-value: {results['p-value']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluationModule(name: \"accuracy\", module_type: \"metric\", features: {'predictions': Value(dtype='int32', id=None), 'references': Value(dtype='int32', id=None)}, usage: \"\"\"\n",
       "Args:\n",
       "    predictions (`list` of `int`): Predicted labels.\n",
       "    references (`list` of `int`): Ground truth labels.\n",
       "    normalize (`boolean`): If set to False, returns the number of correctly classified samples. Otherwise, returns the fraction of correctly classified samples. Defaults to True.\n",
       "    sample_weight (`list` of `float`): Sample weights Defaults to None.\n",
       "\n",
       "Returns:\n",
       "    accuracy (`float` or `int`): Accuracy score. Minimum possible value is 0. Maximum possible value is 1.0, or the number of examples input, if `normalize` is set to `True`.. A higher score means higher accuracy.\n",
       "\n",
       "Examples:\n",
       "\n",
       "    Example 1-A simple example\n",
       "        >>> accuracy_metric = evaluate.load(\"accuracy\")\n",
       "        >>> results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0])\n",
       "        >>> print(results)\n",
       "        {'accuracy': 0.5}\n",
       "\n",
       "    Example 2-The same as Example 1, except with `normalize` set to `False`.\n",
       "        >>> accuracy_metric = evaluate.load(\"accuracy\")\n",
       "        >>> results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0], normalize=False)\n",
       "        >>> print(results)\n",
       "        {'accuracy': 3.0}\n",
       "\n",
       "    Example 3-The same as Example 1, except with `sample_weight` set.\n",
       "        >>> accuracy_metric = evaluate.load(\"accuracy\")\n",
       "        >>> results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0], sample_weight=[0.5, 2, 0.7, 0.5, 9, 0.4])\n",
       "        >>> print(results)\n",
       "        {'accuracy': 0.8778625954198473}\n",
       "\"\"\", stored examples: 0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.5}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = acc.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch [1/3]:   0%|          | 0/563 [00:00<?, ?it/s]/home/didiersalest/NLP_HW3_2/NLP-Exercise-3/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 16, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "Training epoch [1/3]: 100%|██████████| 563/563 [00:40<00:00, 13.88it/s, loss=0.534]\n",
      "Validation epoch [1/3]: 100%|██████████| 63/63 [00:01<00:00, 39.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch no. 0 - Pearson Correlation: 0.896378647662928 - Accuracy: 0.876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch [2/3]:   0%|          | 0/563 [00:00<?, ?it/s]/home/didiersalest/NLP_HW3_2/NLP-Exercise-3/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 16, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "Training epoch [2/3]: 100%|██████████| 563/563 [00:39<00:00, 14.09it/s, loss=0.169]\n",
      "Validation epoch [2/3]: 100%|██████████| 63/63 [00:01<00:00, 38.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch no. 1 - Pearson Correlation: 0.8616918713520976 - Accuracy: 0.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch [3/3]:   0%|          | 0/563 [00:00<?, ?it/s]/home/didiersalest/NLP_HW3_2/NLP-Exercise-3/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 16, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "Training epoch [3/3]: 100%|██████████| 563/563 [00:39<00:00, 14.34it/s, loss=0.452]\n",
      "Validation epoch [3/3]: 100%|██████████| 63/63 [00:01<00:00, 38.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch no. 2 - Pearson Correlation: 0.8864331895514849 - Accuracy: 0.89\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "best_score = 0.0\n",
    "\n",
    "for ep in range(epochs):\n",
    "    batch_train_index=0\n",
    "    pbar = tqdm(dl_train)\n",
    "    pbar.set_description(f\"Training epoch [{ep+1}/{epochs}]\")\n",
    "    model.train()\n",
    "    # TODO4: Write the training loop\n",
    "    # Write your code here\n",
    "    # train your model\n",
    "    # clear gradient\n",
    "    # forward pass\n",
    "    # compute loss\n",
    "    # back-propagation\n",
    "    # model optimization\n",
    "    for input_batch, rel_score_batch, entail_judge_batch in pbar:\n",
    "        input_batch = input_batch.to(device)\n",
    "        rel_score_batch = rel_score_batch.to(device)\n",
    "        entail_judge_batch = entail_judge_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        rel_score_preds, entail_judge_preds = model(**input_batch)\n",
    "\n",
    "        regression_loss = regression_criterion(rel_score_preds.squeeze(), rel_score_batch)\n",
    "        classification_loss = classification_criterion(entail_judge_preds, entail_judge_batch)\n",
    "        \n",
    "        overall_loss = regression_loss + classification_loss\n",
    "\n",
    "        overall_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_train_index+=1\n",
    "        if batch_train_index%50==0:\n",
    "            pbar.set_postfix(loss = overall_loss.item())\n",
    "\n",
    "\n",
    "    pbar = tqdm(dl_validation)\n",
    "    pbar.set_description(f\"Validation epoch [{ep+1}/{epochs}]\")\n",
    "    model.eval()\n",
    "    # TODO5: Write the evaluation loop\n",
    "    # Write your code here\n",
    "    # Evaluate your model\n",
    "    # Output all the evaluation scores (PearsonCorr, Accuracy)\n",
    "    real_rel_scores = []\n",
    "    pred_rel_scores = []\n",
    "    real_entail_classes = []\n",
    "    pred_entail_classes = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        batch_val_index=0\n",
    "        for input_batch, rel_score_real_batch, entail_judge_real_batch in pbar:\n",
    "            input_batch = input_batch.to(device)\n",
    "            rel_score_real_batch = rel_score_real_batch.to(device)\n",
    "            entail_judge_real_batch = entail_judge_real_batch.to(device)\n",
    "\n",
    "            rel_score_pred_batch, entail_judge_pred_batch = model(**input_batch)\n",
    "\n",
    "            entailment_predicted_labels = torch.argmax(entail_judge_pred_batch, dim=1)\n",
    "\n",
    "            pred_rel_scores.append(rel_score_pred_batch.cpu())\n",
    "            real_rel_scores.append(rel_score_real_batch.cpu())\n",
    "            pred_entail_classes.append(entailment_predicted_labels.cpu())\n",
    "            real_entail_classes.append(entail_judge_real_batch.cpu())\n",
    "\n",
    "        pred_rel_scores = torch.cat(pred_rel_scores).squeeze()\n",
    "        real_rel_scores = torch.cat(real_rel_scores)\n",
    "        pred_entail_classes = torch.cat(pred_entail_classes)\n",
    "        real_entail_classes = torch.cat(real_entail_classes)\n",
    "\n",
    "        pearson_corr = psr.compute(references = real_rel_scores, predictions = pred_rel_scores)['pearsonr'] # Write your code here\n",
    "        accuracy = acc.compute(references=real_entail_classes, predictions=pred_entail_classes)['accuracy'] # Write your code here\n",
    "        # print(f\"F1 Score: {f1.compute()}\")\n",
    "        # batch_val_index+=1\n",
    "        # if batch_val_index%10==0:\n",
    "        print(f\"Epoch no. {ep} - Pearson Correlation: {pearson_corr} - Accuracy: {accuracy}\")\n",
    "        if pearson_corr + accuracy > best_score:\n",
    "            best_score = pearson_corr + accuracy\n",
    "            os.makedirs(\"./saved_models\", exist_ok=True)\n",
    "            torch.save(model.state_dict(), f'./saved_models/best_model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Test:   0%|          | 0/616 [00:00<?, ?it/s]/home/didiersalest/NLP_HW3_2/NLP-Exercise-3/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 16, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "Test: 100%|██████████| 616/616 [00:09<00:00, 66.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set - Pearson Correlation: 0.8888255144218109 - Accuracy: 0.901968743657398\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model = MultiLabelModel().to(device)\n",
    "model.load_state_dict(torch.load(f\"./saved_models/best_model.ckpt\", weights_only=True))\n",
    "\n",
    "# Test Loop\n",
    "pbar = tqdm(dl_test, desc=\"Test\")\n",
    "model.eval()\n",
    "\n",
    "# TODO6: Write the test loop\n",
    "# Write your code here\n",
    "# We have loaded the best model with the highest evaluation score for you\n",
    "# Please implement the test loop to evaluate the model on the test dataset\n",
    "# We will have 10% of the total score for the test accuracy and pearson correlation\n",
    "real_rel_scores = []\n",
    "pred_rel_scores = []\n",
    "real_entail_classes = []\n",
    "pred_entail_classes = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    batch_val_index=0\n",
    "    for input_batch, rel_score_real_batch, entail_judge_real_batch in pbar:\n",
    "        input_batch = input_batch.to(device)\n",
    "        rel_score_real_batch = rel_score_real_batch.to(device)\n",
    "        entail_judge_real_batch = entail_judge_real_batch.to(device)\n",
    "\n",
    "        rel_score_pred_batch, entail_judge_pred_batch = model(**input_batch)\n",
    "\n",
    "        entailment_predicted_labels = torch.argmax(entail_judge_pred_batch, dim=1)\n",
    "\n",
    "        pred_rel_scores.append(rel_score_pred_batch.cpu())\n",
    "        real_rel_scores.append(rel_score_real_batch.cpu())\n",
    "        pred_entail_classes.append(entailment_predicted_labels.cpu())\n",
    "        real_entail_classes.append(entail_judge_real_batch.cpu())\n",
    "\n",
    "    pred_rel_scores = torch.cat(pred_rel_scores).squeeze()\n",
    "    real_rel_scores = torch.cat(real_rel_scores)\n",
    "    pred_entail_classes = torch.cat(pred_entail_classes)\n",
    "    real_entail_classes = torch.cat(real_entail_classes)\n",
    "\n",
    "    pearson_corr = psr.compute(references = real_rel_scores, predictions = pred_rel_scores)['pearsonr']\n",
    "    accuracy = acc.compute(references=real_entail_classes, predictions=pred_entail_classes)['accuracy']\n",
    "   \n",
    "    print(f\"Test Set - Pearson Correlation: {pearson_corr} - Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp-exercise-3_2)",
   "language": "python",
   "name": "nlp-exercise-3_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
