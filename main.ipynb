{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "from datasets import load_dataset\n",
    "from evaluate import load\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#  You can install and import any other libraries if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some Chinese punctuations will be tokenized as [UNK], so we replace them with English ones\n",
    "token_replacement = [\n",
    "    [\"：\" , \":\"],\n",
    "    [\"，\" , \",\"],\n",
    "    [\"“\" , \"\\\"\"],\n",
    "    [\"”\" , \"\\\"\"],\n",
    "    [\"？\" , \"?\"],\n",
    "    [\"……\" , \"...\"],\n",
    "    [\"！\" , \"!\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\", cache_dir=\"./cache/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 5604, 1996, 6434, 1012,  102, 5604, 2117, 6251, 1012,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(text=\"Testing the output.\", text_pair=\"Testing second sentence.\", padding = True, truncation=True, return_tensors=\"pt\", return_attention_mask=True, return_token_type_ids=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 5.20k/5.20k [00:00<00:00, 13.1kB/s]\n",
      "Downloading readme: 100%|██████████| 3.56k/3.56k [00:00<00:00, 8.42kB/s]\n",
      "Downloading data: 100%|██████████| 87.3k/87.3k [00:00<00:00, 124kB/s] \n",
      "Downloading data: 100%|██████████| 93.4k/93.4k [00:00<00:00, 135kB/s] \n",
      "Downloading data: 100%|██████████| 16.4k/16.4k [00:00<00:00, 49.0MB/s]\n",
      "Generating train split: 100%|██████████| 4500/4500 [00:00<00:00, 10527.71 examples/s]\n",
      "Generating test split: 100%|██████████| 4927/4927 [00:00<00:00, 62417.76 examples/s]\n",
      "Generating validation split: 100%|██████████| 500/500 [00:00<00:00, 48386.14 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset example: \n",
      "{'sentence_pair_id': 1, 'premise': 'A group of kids is playing in a yard and an old man is standing in the background', 'hypothesis': 'A group of boys in a yard is playing and a man is standing in the background', 'relatedness_score': 4.5, 'entailment_judgment': 0} \n",
      "{'sentence_pair_id': 2, 'premise': 'A group of children is playing in the house and there is no man standing in the background', 'hypothesis': 'A group of kids is playing in a yard and an old man is standing in the background', 'relatedness_score': 3.200000047683716, 'entailment_judgment': 0} \n",
      "{'sentence_pair_id': 3, 'premise': 'The young boys are playing outdoors and the man is smiling nearby', 'hypothesis': 'The kids are playing outdoors near a man with a smile', 'relatedness_score': 4.699999809265137, 'entailment_judgment': 1}\n"
     ]
    }
   ],
   "source": [
    "class SemevalDataset(Dataset):\n",
    "    def __init__(self, split=\"train\") -> None:\n",
    "        super().__init__()\n",
    "        assert split in [\"train\", \"validation\", \"test\"]\n",
    "        self.data = load_dataset(\n",
    "            \"sem_eval_2014_task_1\", split=split, trust_remote_code=True, cache_dir=\"./cache/\"\n",
    "        ).to_list()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        d = self.data[index]\n",
    "        # Replace Chinese punctuations with English ones\n",
    "        for k in [\"premise\", \"hypothesis\"]:\n",
    "            for tok in token_replacement:\n",
    "                d[k] = d[k].replace(tok[0], tok[1])\n",
    "        return d\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "data_sample = SemevalDataset(split=\"train\").data[:3]\n",
    "print(f\"Dataset example: \\n{data_sample[0]} \\n{data_sample[1]} \\n{data_sample[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters\n",
    "# You can modify these values if needed\n",
    "lr = 3e-5\n",
    "epochs = 3\n",
    "train_batch_size = 8\n",
    "validation_batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO1: Create batched data for DataLoader\n",
    "# `collate_fn` is a function that defines how the data batch should be packed.\n",
    "# This function will be called in the DataLoader to pack the data batch.\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # TODO1-1: Implement the collate_fn function\n",
    "    # Write your code here\n",
    "    # The input parameter is a data batch (tuple), and this function packs it into tensors.\n",
    "    # Use tokenizer to pack tokenize and pack the data and its corresponding labels.\n",
    "    # Return the data batch and labels for each sub-task.\n",
    "    premises = [data_instance[\"premise\"] for data_instance in batch]\n",
    "    hypothesis = [data_instance[\"hypothesis\"] for data_instance in batch]\n",
    "    relatedness_scores = [data_instance[\"relatedness_score\"] for data_instance in batch]\n",
    "    entailment_judgments = [data_instance[\"entailment_judgment\"] for data_instance in batch]\n",
    "\n",
    "    input_texts = tokenizer(text=premises, text_pair=hypothesis, padding = True, truncation=True, return_tensors=\"pt\", return_attention_mask=True, return_token_type_ids=True)\n",
    "\n",
    "    relatedness_scores=torch.FloatTensor(relatedness_scores)\n",
    "    entailment_judgments=torch.LongTensor(entailment_judgments)\n",
    "\n",
    "    return input_texts, relatedness_scores, entailment_judgments\n",
    "\n",
    "# TODO1-2: Define your DataLoader\n",
    "dl_train = torch.utils.data.DataLoader(dataset=SemevalDataset(split=\"train\"), collate_fn=collate_fn, batch_size=train_batch_size, shuffle=True, num_workers=32) # Write your code here\n",
    "dl_validation = torch.utils.data.DataLoader(dataset=SemevalDataset(split=\"validation\"), collate_fn=collate_fn, batch_size=validation_batch_size, shuffle=False, num_workers=32)  # Write your code here\n",
    "dl_test = torch.utils.data.DataLoader(dataset=SemevalDataset(split=\"test\"), collate_fn=collate_fn, batch_size=validation_batch_size, shuffle=False, num_workers=32) # Write your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'input_ids': tensor([[  101,  1037,  9081,  1999,  2665,  2003,  3564,  2011,  1037, 11820,\n",
      "          2879,   102,  1037,  9081,  2879,  1999,  2665,  2003,  3564,  2006,\n",
      "          1037,  7370,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0],\n",
      "        [  101,  1037, 12170,  5666, 20464,  2923,  2003,  3173,  1037,  7997,\n",
      "          2058,  2010,  2132,  1999,  1037,  2177,  1997,  2111,   102,  1037,\n",
      "         10459, 12170,  5666, 20464,  2923,  2003,  5128,  2091,  1037,  7997,\n",
      "           102],\n",
      "        [  101,  1037,  2316,  2003,  2652,  2070,  5693,   102,  1037,  2158,\n",
      "          2003,  2652,  2019,  4816,  9019,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0],\n",
      "        [  101,  1037,  2711,  1999, 28899,  6718,  2003,  3061, 11328,  1999,\n",
      "          2392,  1997,  2070,  4020,   102,  1996,  3137,  6718,  2003, 11328,\n",
      "          2872,  1999,  2392,  1997,  1996,  3061, 28988,   102,     0,     0,\n",
      "             0],\n",
      "        [  101,  1037,  4744,  2003,  2855,  2183,  2091,  1037,  2940,   102,\n",
      "          1037,  4744,  2003,  2855,  2183,  2039,  1037,  2940,   102,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0],\n",
      "        [  101,  1037,  2450,  2003,  2559,  2012,  1996,  3193,  1997,  1037,\n",
      "          2103,   102,  1996,  2450,  2003,  2559,  2012,  1996,  3193,  1997,\n",
      "          1037,  2103,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0],\n",
      "        [  101,  1037,  2450,  2003, 14744,  2075,  3869,   102,  1037,  2450,\n",
      "          2003,  2025, 14744,  2075,  3869,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0],\n",
      "        [  101,  1037,  2158,  2003,  3564,  2006,  1037,  3345,  1998,  8345,\n",
      "          2010,  2192,  2114,  2010,  2227,   102,  1037,  2158,  2003,  3061,\n",
      "          2006,  1037,  3345,  1998,  8345,  2010,  2192,  2006,  2010,  5001,\n",
      "           102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1]])}, tensor([3.1000, 3.1000, 3.7000, 3.6000, 4.0000, 4.9000, 3.3000, 3.4000]), tensor([0, 2, 0, 0, 0, 1, 2, 0]))\n"
     ]
    }
   ],
   "source": [
    "print(next(iter(dl_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO2: Construct your model\n",
    "class MultiLabelModel(torch.nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "        # Write your code here\n",
    "        # Define what modules you will use in the model\n",
    "        # Please use \"google-bert/bert-base-uncased\" model (https://huggingface.co/google-bert/bert-base-uncased)\n",
    "        # Besides the base model, you may design additional architectures by incorporating linear layers, activation functions, or other neural components.\n",
    "        # Remark: The use of any additional pretrained language models is not permitted.\n",
    "\n",
    "        num_classes = 3 # Based on our data\n",
    "        self.bert_model = BertModel.from_pretrained(\"google-bert/bert-base-uncased\", cache_dir=\"./cache/\")\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "\n",
    "        self.regression_head = torch.nn.Linear(self.bert_model.config.hidden_size,1)\n",
    "        self.classification_head=torch.nn.Linear(self.bert_model.config.hidden_size,num_classes)\n",
    "        \n",
    "    def forward(self, **kwargs):\n",
    "        # Write your code here\n",
    "        # Forward pass\n",
    "\n",
    "        output=self.bert_model(input_ids=kwargs.get(\"input_ids\"), \n",
    "                               token_type_ids=kwargs.get(\"token_type_ids\"), \n",
    "                               attention_mask=kwargs.get(\"attention_mask\"))\n",
    "        output_dropout = self.dropout(output.pooler_output)\n",
    "        output_regression = self.regression_head(output_dropout)\n",
    "        output_classification = self.classification_head(output_dropout)\n",
    "\n",
    "        return output_regression, output_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO3: Define your optimizer and loss function\n",
    "\n",
    "model = MultiLabelModel().to(device)\n",
    "# TODO3-1: Define your Optimizer\n",
    "optimizer = torch.optim.AdamW(params =  model.parameters(), lr=lr) # Write your code here\n",
    "\n",
    "# TODO3-2: Define your loss functions (you should have two)\n",
    "# Write your code here\n",
    "classification_criterion = torch.nn.CrossEntropyLoss()\n",
    "regression_criterion = torch.nn.MSELoss()\n",
    "\n",
    "# scoring functions\n",
    "psr = load(\"pearsonr\")\n",
    "acc = load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluationModule(name: \"pearsonr\", module_type: \"metric\", features: {'predictions': Value(dtype='float32', id=None), 'references': Value(dtype='float32', id=None)}, usage: \"\"\"\n",
       "Args:\n",
       "    predictions (`list` of `int`): Predicted class labels, as returned by a model.\n",
       "    references (`list` of `int`): Ground truth labels.\n",
       "    return_pvalue (`boolean`): If `True`, returns the p-value, along with the correlation coefficient. If `False`, returns only the correlation coefficient. Defaults to `False`.\n",
       "\n",
       "Returns:\n",
       "    pearsonr (`float`): Pearson correlation coefficient. Minimum possible value is -1. Maximum possible value is 1. Values of 1 and -1 indicate exact linear positive and negative relationships, respectively. A value of 0 implies no correlation.\n",
       "    p-value (`float`): P-value, which roughly indicates the probability of an The p-value roughly indicates the probability of an uncorrelated system producing datasets that have a Pearson correlation at least as extreme as the one computed from these datasets. Minimum possible value is 0. Maximum possible value is 1. Higher values indicate higher probabilities.\n",
       "\n",
       "Examples:\n",
       "\n",
       "    Example 1-A simple example using only predictions and references.\n",
       "        >>> pearsonr_metric = evaluate.load(\"pearsonr\")\n",
       "        >>> results = pearsonr_metric.compute(predictions=[10, 9, 2.5, 6, 4], references=[1, 2, 3, 4, 5])\n",
       "        >>> print(round(results['pearsonr'], 2))\n",
       "        -0.74\n",
       "\n",
       "    Example 2-The same as Example 1, but that also returns the `p-value`.\n",
       "        >>> pearsonr_metric = evaluate.load(\"pearsonr\")\n",
       "        >>> results = pearsonr_metric.compute(predictions=[10, 9, 2.5, 6, 4], references=[1, 2, 3, 4, 5], return_pvalue=True)\n",
       "        >>> print(sorted(list(results.keys())))\n",
       "        ['p-value', 'pearsonr']\n",
       "        >>> print(round(results['pearsonr'], 2))\n",
       "        -0.74\n",
       "        >>> print(round(results['p-value'], 2))\n",
       "        0.15\n",
       "\"\"\", stored examples: 0)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pearsonr': np.float64(0.9997647888947827), 'p-value': np.float64(4.330173202906213e-06)}\n",
      "Pearson correlation coefficient (r): 1.000\n",
      "P-value: 0.000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.array([10, 12, 15, 18, 20])\n",
    "y = np.array([25, 30, 38, 45, 50])\n",
    "# Calculate Pearson correlation and p-value\n",
    "results = psr.compute(references = x, predictions = y, return_pvalue=True)\n",
    "print(results)\n",
    "# Print the results\n",
    "print(f\"Pearson correlation coefficient (r): {results['pearsonr']:.3f}\")\n",
    "print(f\"P-value: {results['p-value']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluationModule(name: \"accuracy\", module_type: \"metric\", features: {'predictions': Value(dtype='int32', id=None), 'references': Value(dtype='int32', id=None)}, usage: \"\"\"\n",
       "Args:\n",
       "    predictions (`list` of `int`): Predicted labels.\n",
       "    references (`list` of `int`): Ground truth labels.\n",
       "    normalize (`boolean`): If set to False, returns the number of correctly classified samples. Otherwise, returns the fraction of correctly classified samples. Defaults to True.\n",
       "    sample_weight (`list` of `float`): Sample weights Defaults to None.\n",
       "\n",
       "Returns:\n",
       "    accuracy (`float` or `int`): Accuracy score. Minimum possible value is 0. Maximum possible value is 1.0, or the number of examples input, if `normalize` is set to `True`.. A higher score means higher accuracy.\n",
       "\n",
       "Examples:\n",
       "\n",
       "    Example 1-A simple example\n",
       "        >>> accuracy_metric = evaluate.load(\"accuracy\")\n",
       "        >>> results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0])\n",
       "        >>> print(results)\n",
       "        {'accuracy': 0.5}\n",
       "\n",
       "    Example 2-The same as Example 1, except with `normalize` set to `False`.\n",
       "        >>> accuracy_metric = evaluate.load(\"accuracy\")\n",
       "        >>> results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0], normalize=False)\n",
       "        >>> print(results)\n",
       "        {'accuracy': 3.0}\n",
       "\n",
       "    Example 3-The same as Example 1, except with `sample_weight` set.\n",
       "        >>> accuracy_metric = evaluate.load(\"accuracy\")\n",
       "        >>> results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0], sample_weight=[0.5, 2, 0.7, 0.5, 9, 0.4])\n",
       "        >>> print(results)\n",
       "        {'accuracy': 0.8778625954198473}\n",
       "\"\"\", stored examples: 0)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.5}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = acc.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch [1/3]: 100%|██████████| 563/563 [00:23<00:00, 23.85it/s, loss=0.0744]\n",
      "Validation epoch [1/3]: 100%|██████████| 63/63 [00:01<00:00, 48.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch no. 0\t-\tPearson Correlation: 0.8816792126808541\t-\tAccuracy: 0.868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch [2/3]: 100%|██████████| 563/563 [00:23<00:00, 24.00it/s, loss=0.108] \n",
      "Validation epoch [2/3]: 100%|██████████| 63/63 [00:01<00:00, 48.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch no. 0\t-\tPearson Correlation: 0.8837566349077156\t-\tAccuracy: 0.876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch [3/3]: 100%|██████████| 563/563 [00:23<00:00, 23.80it/s, loss=0.0832]\n",
      "Validation epoch [3/3]: 100%|██████████| 63/63 [00:01<00:00, 47.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch no. 0\t-\tPearson Correlation: 0.8743169218029911\t-\tAccuracy: 0.876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "best_score = 0.0\n",
    "\n",
    "for ep in range(epochs):\n",
    "    batch_train_index=0\n",
    "    pbar = tqdm(dl_train)\n",
    "    pbar.set_description(f\"Training epoch [{ep+1}/{epochs}]\")\n",
    "    model.train()\n",
    "    # TODO4: Write the training loop\n",
    "    # Write your code here\n",
    "    # train your model\n",
    "    # clear gradient\n",
    "    # forward pass\n",
    "    # compute loss\n",
    "    # back-propagation\n",
    "    # model optimization\n",
    "    for input_batch, rel_score_batch, entail_judge_batch in pbar:\n",
    "        input_batch = input_batch.to(device)\n",
    "        rel_score_batch = rel_score_batch.to(device)\n",
    "        entail_judge_batch = entail_judge_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        rel_score_preds, entail_judge_preds = model(**input_batch)\n",
    "\n",
    "        regression_loss = regression_criterion(rel_score_preds.squeeze(), rel_score_batch)\n",
    "        classification_loss = classification_criterion(entail_judge_preds, entail_judge_batch)\n",
    "        \n",
    "        overall_loss = regression_loss + classification_loss\n",
    "\n",
    "        overall_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_train_index+=1\n",
    "        if batch_train_index%50==0:\n",
    "            pbar.set_postfix(loss = overall_loss.item())\n",
    "\n",
    "\n",
    "    pbar = tqdm(dl_validation)\n",
    "    pbar.set_description(f\"Validation epoch [{ep+1}/{epochs}]\")\n",
    "    model.eval()\n",
    "    # TODO5: Write the evaluation loop\n",
    "    # Write your code here\n",
    "    # Evaluate your model\n",
    "    # Output all the evaluation scores (PearsonCorr, Accuracy)\n",
    "    real_rel_scores = []\n",
    "    pred_rel_scores = []\n",
    "    real_entail_classes = []\n",
    "    pred_entail_classes = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        batch_val_index=0\n",
    "        for input_batch, rel_score_real_batch, entail_judge_real_batch in pbar:\n",
    "            input_batch = input_batch.to(device)\n",
    "            rel_score_real_batch = rel_score_real_batch.to(device)\n",
    "            entail_judge_real_batch = entail_judge_real_batch.to(device)\n",
    "\n",
    "            rel_score_pred_batch, entail_judge_pred_batch = model(**input_batch)\n",
    "\n",
    "            entailment_predicted_labels = torch.argmax(entail_judge_pred_batch, dim=1)\n",
    "\n",
    "            pred_rel_scores.append(rel_score_pred_batch.cpu())\n",
    "            real_rel_scores.append(rel_score_real_batch.cpu())\n",
    "            pred_entail_classes.append(entailment_predicted_labels.cpu())\n",
    "            real_entail_classes.append(entail_judge_real_batch.cpu())\n",
    "\n",
    "        pred_rel_scores = torch.cat(pred_rel_scores).squeeze()\n",
    "        real_rel_scores = torch.cat(real_rel_scores)\n",
    "        pred_entail_classes = torch.cat(pred_entail_classes)\n",
    "        real_entail_classes = torch.cat(real_entail_classes)\n",
    "\n",
    "        pearson_corr = psr.compute(references = real_rel_scores, predictions = pred_rel_scores)['pearsonr'] # Write your code here\n",
    "        accuracy = acc.compute(references=real_entail_classes, predictions=pred_entail_classes)['accuracy'] # Write your code here\n",
    "        # print(f\"F1 Score: {f1.compute()}\")\n",
    "        # batch_val_index+=1\n",
    "        # if batch_val_index%10==0:\n",
    "        print(f\"Epoch no. {ep} - Pearson Correlation: {pearson_corr} - Accuracy: {accuracy}\")\n",
    "        if pearson_corr + accuracy > best_score:\n",
    "            best_score = pearson_corr + accuracy\n",
    "            torch.save(model.state_dict(), f'./saved_models/best_model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|██████████| 616/616 [00:05<00:00, 108.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set - Pearson Correlation: 0.8856948854754875 - Accuracy: 0.8727420336919017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model = MultiLabelModel().to(device)\n",
    "model.load_state_dict(torch.load(f\"./saved_models/best_model.ckpt\", weights_only=True))\n",
    "\n",
    "# Test Loop\n",
    "pbar = tqdm(dl_test, desc=\"Test\")\n",
    "model.eval()\n",
    "\n",
    "# TODO6: Write the test loop\n",
    "# Write your code here\n",
    "# We have loaded the best model with the highest evaluation score for you\n",
    "# Please implement the test loop to evaluate the model on the test dataset\n",
    "# We will have 10% of the total score for the test accuracy and pearson correlation\n",
    "real_rel_scores = []\n",
    "pred_rel_scores = []\n",
    "real_entail_classes = []\n",
    "pred_entail_classes = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    batch_val_index=0\n",
    "    for input_batch, rel_score_real_batch, entail_judge_real_batch in pbar:\n",
    "        input_batch = input_batch.to(device)\n",
    "        rel_score_real_batch = rel_score_real_batch.to(device)\n",
    "        entail_judge_real_batch = entail_judge_real_batch.to(device)\n",
    "\n",
    "        rel_score_pred_batch, entail_judge_pred_batch = model(**input_batch)\n",
    "\n",
    "        entailment_predicted_labels = torch.argmax(entail_judge_pred_batch, dim=1)\n",
    "\n",
    "        pred_rel_scores.append(rel_score_pred_batch.cpu())\n",
    "        real_rel_scores.append(rel_score_real_batch.cpu())\n",
    "        pred_entail_classes.append(entailment_predicted_labels.cpu())\n",
    "        real_entail_classes.append(entail_judge_real_batch.cpu())\n",
    "\n",
    "    pred_rel_scores = torch.cat(pred_rel_scores).squeeze()\n",
    "    real_rel_scores = torch.cat(real_rel_scores)\n",
    "    pred_entail_classes = torch.cat(pred_entail_classes)\n",
    "    real_entail_classes = torch.cat(real_entail_classes)\n",
    "\n",
    "    pearson_corr = psr.compute(references = real_rel_scores, predictions = pred_rel_scores)['pearsonr']\n",
    "    accuracy = acc.compute(references=real_entail_classes, predictions=pred_entail_classes)['accuracy']\n",
    "   \n",
    "    print(f\"Test Set - Pearson Correlation: {pearson_corr} - Accuracy: {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp-exercise-3)",
   "language": "python",
   "name": "nlp-exercise-3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
